from fastapi import FastAPI
from pydantic import BaseModel
from LLM.echo import generate_echo
from fastapi.middleware.cors import CORSMiddleware 

app = FastAPI()

origins = [
    # Allow the specific URL where your extension content script runs.
    # For a browser extension targeting AI platforms, you need to allow
    # the domains of those platforms.
    "https://chatgpt.com",
    "https://gemini.google.com",
    "https://claude.ai",
    
    # If testing locally, you might need to allow localhost:
    "http://localhost",
    "http://localhost:8000",
    "http://127.0.0.1:8000",
    
    # You can also use "*" to allow ALL origins, but this is less secure.
    # For a browser extension accessing public domains, using specific domains is better.
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,             
    allow_credentials=True,           
    allow_methods=["*"],              
    allow_headers=["*"],              
)

class InputData(BaseModel):
    text: str

def custom_prompt(data: str):
    prompt = f'''
You are the backend summarization engine for **CrossAI**, a browser extension that transfers
context between different AI platforms (ChatGPT → Claude → Gemini → etc.).

IMPORTANT:
You will sometimes receive additional context generated by a **previous LLM platform**.
This context reflects what the previous model already understood about the conversation.
Your first job is to:
0. **Ingest and consolidate any prior context provided**, and merge it seamlessly with the
new conversation data. Treat the previous context as reliable and integrate it so the
final summary reflects the full story across platforms.

Your task:
- You will receive a raw text dump of a conversation in the variable called `data`.
- This dump contains **multiple chat nodes**, but they are not explicitly separated.
- A chat node looks like:

    user question: <text>
    assistance response: <text>

- Multiple such nodes may be concatenated one after another inside `data`.

Your job:
1. **Parse** the raw text and correctly identify all chat nodes.
2. **Reconstruct** the conversation structure in chronological order.
3. **Summarize the entire conversation** (including any prior LLM context) into a concise but
accurate context block that can be given to other LLMs so they understand everything
that happened so far across platforms.
4. The summary should:
- Preserve all important user intentions, tasks, requests, constraints, preferences, and goals.
- Preserve all important assistant explanations, logic, solutions, or decisions.
- Be neutral, factual, and free of hallucinations.
- NOT rewrite or paraphrase the full conversation; instead produce a clean context summary.
- Expand only as much as needed depending on conversation length.
    (Short convo → shorter summary. Long convo → proportionally more compressed.)
- Stay well under multi-million token windows and avoid verbosity.

5. Remove:
- Filler conversational fluff.
- Repeated confirmations.
- Irrelevant metadata or greetings.

6. Output format:
You must output ONLY the final high-quality merged summary.
No analysis, no extra explanations, no reflections, no instructions.

Now here is the raw conversation text that you must process:

--------------------
{data}
--------------------

Produce the final summarized context now.
'''
    return prompt


@app.post("/generate_echo")
def generate_result(data: InputData):
    prompt = custom_prompt(data.text)
    result = generate_echo(prompt)
    return result
